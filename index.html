<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <title>Kazam!</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
</head>
<body>
  <header>
    <nav class="blue lighten-1" role="navigation">
      <div class="nav-wrapper container"><a id="logo-container" href="./index.html" class="brand-logo">15-418 Final Project</a>
        <ul class="right hide-on-med-and-down">
          <li><a href="./proposal.html">Proposal</a></li>
          <li><a href="./checkpoint.html">Check Point</a></li>
          <li class="active"><a href="#">Final Write-Up</a></li>
        </ul>
  
        <ul id="nav-mobile" class="side-nav">
          <li><a href="./proposal.html">Proposal</a></li>
          <li><a href="./checkpoint.html">Check Point</a></li>
          <li class="active"><a href="#">Final Write-Up</a></li>
        </ul>
        <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
      </div>
    </nav>
  </header>

  <main>
    
    <div class="container">
      <div class="section">
        <div class="row">
          <div class="col s12 m12">
            <h3 class="header center">Kazam!</h3>
            <h5 class="center">Parallel Audio Recognition</h5>
            <h5 class="center">Final Thoughts & Results</h5>
            <p class="center">Casey Fischer and Udaya Malik</p> 
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="summary" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <h4>Project Summary</h4>
            <p><a href="https://github.com/caseyfisch/15418-final">Kazam</a> is a twist on the implementation of Shazam's music recognition
            algorithm.  We implemented a rudimentary and simplified version of the 
            audio fingerprinting algorithm to uniquely identify input audio files
            based on the key frequencies that occur throughout the song.  We 
            optimized our algorithm by modifying the algorithm proposed in the 
            paper published by one of Shazam's co-founders, as well as eliminating
            the constraint to process captured audio in real time.  Doing so allowed
            us to explore how much we could optimize the search process with a 
            unique audio fingerprinting approach.  We used OpenMP to attempt to 
            speed up our algorithm on GHC Machine 45, and we
            analyzed performance with a variable number of
            threads, as well as varying sizes for our database of songs.</p>
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="background" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <h4>Background</h4>
            <p>Audio fingerprinting is a process that translates audio signals into
            a digital summary of the audio sample, usually by transforming the 
            analog, continuous signal in the time domain to a digital, discrete 
            representation of the signal in the frequency domain.  If two audio samples
            sound the same to the human ear, then their audio fingerprints should
            be the same as well.  Audio fingerprints are analogous to human fingerprints,
            in the sense that they should retain some of their unique characteristics,
            even in the presence of noise or distortion.  Thus, by comparing the 
            audio fingerprints of two samples, we can determine whether or not
            they correspond to the same original audio.  However, we can't just 
            compare the raw audio data.  Just because two audio signals sound the same,
            doesn't mean that they'll necessarily have the same digital binary 
            representation.  This is the primary motivation behind creating 
            audio fingerprints.</p>
            
            <p>Shazam developed a clever algorithm to robustly and efficiently
            identify captured audio, in real time, by creating an audio fingerprint 
            of the input audio signal, and comparing that fingerprint with
            the fingerprints of all the songs that comprise their massive database
            of music.  The algorithm at a high level is rather simple: it captures
            a stream of audio signals, converts discrete blocks of signals from
            the time domain to the frequency domain, finds the most prominent 
            frequencies that occur during that time frame, and cleverly construct
            hashcodes to represent where those unique frequencies occur in time.
            These codes are then simply searched for amongst all the codes for 
            songs that exist in the database to find the most probable match.  The
            Shazam algorithm uses this approach to be robust against noise and
            distortion, as well as to be efficient in the way they compress
            the audio fingerprint of the original songs.</p>
            
            <a href="https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition#from-top-to-bottom" target="toptal"><img class="responsive-img center-block" src="images/pipeline.jpg"></a>

            <p>You can see the described algorithm in the pipeline shown in the 
            figure above.  We decided to adjust the scope of the algorithm, and instead just
            focus on optimizing the matching of audio WAV files against our own
            constructed database of songs.  Our algorithm takes as its input 
            a WAV file, transforms discrete time intervals of the song into
            the frequency domain, and then constructs a different audio fingerprint 
            than the audio fingerprint that Shazam uses.  Since we reduced the 
            scope of the problem to static files and eliminated the real-time 
            restriction, we eliminated the need to implement metrics to combat
            noise.  However, this offered us more flexibility in designing
            our own fingerprinting algorithm.</p>
            
            <p>Here are some of the basic details of our implementation:</p>
            <ol>
              <li><b>Key data structures and operations:</b><p>For each block of audio, we transform 
              its frequency data into a histogram to represent the audio fingerprint 
              for the audio at that point in time.  Normalizing the histogram gives us 
              something similar to a probability density function, which allows
              us to efficiently scan over database songs to find matches.</p></li>
              
              <li><b>Inputs and outputs:</b><p>There are two phases of the algorithm: 
              the WAV file transformation and the audio fingerprint match search.
              </p>
              
              <p>
                Transforming the WAV file takes as input a WAV file and outputs 
                our unique audio fingerprint for the audio file as a simple text 
                file.  Searching for an audio fingerprint match takes as input 
                a database of audio fingerprints and a query audio fingerprint and
                outputs the song in the database that exhibits the highest percentage
                of similarity with the query audio fingerprint.
              </p>
              </li>
              <li>
                <b>Expensive computations:</b> <p>The most computationally expensive
                phase of the algorithm is searching the database for a match.  Since
                there are no direct dependencies between checking a query against
                each individual song in the database, we can easily parallelize
                over each song in the database.  We can also limit the time spent 
                in the search by implementing an early break away mechanism if a 
                particular song exhibits a high level of similarity after a set 
                minimum time limit.</p>
              </li>
              
              <li><b>Workload & Opportunities for Parallelism:</b> There are virtually 
              no dependencies in the sequential version of the algorithm.  However, if 
              we want to implement an early break out mechanism (say, if a thread is 
              calculating the similarity of a query to its database song, and it sees 
              that the song exhibits greater than 90% similarity for longer than ten 
              seconds worth of audio, then we can short circuit the rest of the database 
              execution), then we introduce some synchronization costs and dependencies 
              between threads.  <br /><br />
              Generation of histograms is one part of the algorithm that is greatly 
              amenable to SIMD execution. For every song, we go over the same process 
              of dividing it up into time frames of 2048 samples and then perform 
              the fast fourier transform on them, normalizing them and distributing 
              them within the bins of the histogram based on the frequency distributions. 
              The algorithm has minimal divergence, making it perfect for SIMD utilization. 
              Using SIMD execution, we could perform this sequence of operations on a 
              bunch of songs together, with each song occupying a lane within the vector, 
              and each thread working on that array. </li>
            </ol>
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="approach" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <h4>Approach</h4>
            <p>We originally intended to use a Raspberry Pi with a JTAG to capture 
            audio in real time, perform the necessary FFT to get the frequency representation, 
            and execute the remainder of the fingerprinting and search algorithm.  We 
            were starting from a lab from 18-349, where audio signals were captured, 
            transformed, and the resulting frequencies were used to help tune a guitar.  
            However, we came across several limitations using this real-time sampling approach. 
            We were unable to sample the incoming audio at a rate greater than 4000 samples per 
            second due to the implemented double buffering sample collection mechanism.   
            This sample rate did not provide accurate enough samples to create an accurate 
            representation of the analog audio signals.  For these reasons, we decided to try 
            a simpler approach, which utilizes a combination of a sound file reading library, 
            an FFT C library, and C++.</p>
            
            
            
            </div>
          </div>
          
          <div class="row">
            <div class="col s12 m6"><img class="responsive-img center-block" src="./images/gen-af.png"></div>
            <div class="col s12 m6">
              <p>To simplify the problem space, we decided to do away with recognizing a song 
            in real time and just process static WAV files. We created our database of songs 
            through our own implementation of audio fingerprint generation, which is what we 
            used for our test cases. We generated this database using the following pipeline: 
            We cut down each song to time frames of 2048 samples, and for each time frame, 
            created a normalized histogram. This histogram comprises of 10 bins with frequency 
            distributions from 0 - 1000 Hz, and the value for each bin is determined by the 
            magnitude of the frequencies that fall into the bin.  The normalized histograms 
            represent a pseudo probability density function, indicating the probability of 
            finding a particular frequency at time step i in the audio fingerprint.  We exported 
            this information out to textfiles to comprise our database of audio fingerprints.</p>
              <p>
              We repeat the same process to compute the audio fingerprint for any query sound file. 
              To find the query song within this database, we compare the histograms of the query 
              to those of every song within the database, keeping track of the similarities between 
              every song and the query.  We compute similarity between two fingerprints in the 
              following way: we use a metric similar to the way to determine the divergence between 
              two PDFs.  For each corresponding histogram bin, we accumulate the sum of the minimum 
              of the two values at each bin.  The closer the two values are, the more of a match 
              they are for that given time step.  The similarity value will be 1 in the case of a 
              perfect match, or less than 1 depending on how much of a difference is between the 
              corresponding bin values.  The similarity is then the accumulated bin sum divided 
              by the number of time steps we’ve compared.   In the end, the algorithm returns 
              the song with the highest similarity value.  

            </p>
            </div>
          </div>
          
          <div class="row">
            <div class="col s12 m12">
              <p>To parallelize this approach, we used OpenMP. We parallelized over our search 
              algorithm, where we compare the query song’s audio fingerprint to every song’s 
              audio fingerprint within the database. With OpenMP, we assign each thread to 
              comparing the query song with one song from the database at a time, but we use 
              a static schedule to let OpenMP handle the work assignment.  There are no 
              dependencies within this approach, as the comparison of each song within 
              the database is independent of the other songs. Initially, however, our 
              implementation was extremely I/O bound. We were streaming in information 
              from the text files for both the query song and the songs in the database 
              which made our program I/O bound, resulting in no speedup. This guided us 
              to approach the algorithm a bit differently. Our final approach loads the 
              information from the text files of the songs in the database into the memory 
              of the program each time it is run, allowing us to avoid the I/O overhead 
              during the search. 
              </p>
              
              <p>
                
                We used the Gates machine GHC 45 to test the performance of our algorithm. 
                Since OpenMP essentially spawns threads for each instance of the for-loop 
                iteration for searching through the database, GHC 45 provided the best platform 
                for it.  Note that GHC 45 is a machine with six hyper-threaded cores.

              </p>
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="results" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <h4>Results</h4>
            <p>Here, we present some of the results that each of our optimizations
            provided.</p>
            
            <h5>Performance of Sequential Fingerprint Generation</h5>
            <p>The serial version of our audio fingerprint generation algorithm 
            simply processed an input WAV file, read in blocks of the audio data, 
            performed an FFT on each block, constructed a histogram from the FFT 
            data, and then finally normalized the histogram.  As the figure 
            below shows, the sequential version is already fairly efficient: 
            We can generate the audio fingerprint of a four minute song in about
            half a second.  </p>
            
            <img class="responsive-img center-block" src="./images/s-gen1.png"/>
            
            This seems surprisingly fast, even to perform the FFT's sequentially 
            for all blocks in the song, but we are using an optimized <a href="http://www.fftw.org/" target=blank>FFT  
            library</a> for performing FFT on relatively small arrays.  The focus 
            of our project wasn't to optimize the FFT, which is why we just used 
            this black box library.  However, there is still room for improvement 
            to the generation algorithm.  We essentially have a pipeline that processes
            blocks of the raw WAV audio data.  The work required to transform each
            block into the final histogram is completely independent from any other 
            block, so we can parallelize the process by sending each block of audio 
            data through the pipeline in parallel.  We wanted to have some benchmarks 
            for this algorithm, but unfortunately, some nasty seg faults kept us from
            successfully implementing this optimized approach.
            
            <h5>Performance of Sequential and Parallel Query Matching</h5>
            <p>We optimized the match search of an input audio fingerprint with
            all the fingerprints in our database by parallelizing on the axis of 
            database fingerprints.  We implemented this with OpenMP, and tested
            the performance with a configuration of 12 threads on GHC 45.  The 
            following chart shows a sample of songs of varying lengths, and 
            the amount of time in milliseconds it took to compare it with
            every song in the database (held at a constant size of 40 songs in 
            this case).  This implementation featured no break early mechanism,
            so the entirety of each song was scanned and compared.</p>
            
            <img class="responsive-img center-block" src="./images/svp-query1-1.png"/>
            
            <p>As you can see in the chart, the parallel implementation actually 
            performs marginally worse in all cases than the sequential version.
            We believe this is due to the overhead of spawning threads and using
            them on such a small database, as well as servicing only one query,
            despite loading the entire database.  The search process is very IO intensive,
            given that we have to read in each database file for comparison.  We
            hypothesized that if we increase the database size, then we would see 
            more benefits from the parallelized implementation over the sequential 
            version.  </p>
            
            <img class="responsive-img center-block" src="./images/svp-query1.png"/>
            
            However, as the above graph shows, our prediction was incorrect.  We see 
            that as the size of the database grows, the performance of the parallel 
            implementation is about the same or worse as the performance of the 
            sequential version.  At this point, we realized that our implementation 
            is definitely IO bound.  Even with the parallelism provided by the threads,
            each thread is trying to read its own database file, line by line, which
            generates a lot of contention for the memory bus in the system.  In the
            single-threaded version, this problem doesn't exist because one line of 
            one file is processed at a time.  
            <p>
            Thus, we concluded that it's impractical
            to load the entire database once to search for a single query.  This led
            us to our second approach to test our implementation: loading the entire
            database once, servicing a series of query requests, and then analzying
            the amount of time it takes each implementation to find the most similar 
            song to each query.  The following graph shows the execution time of the 
            sequential and parallel implementations of the search with a constant set of 10 
            queries.  We see that the parallel implementation using OpenMP threads 
            achieves approximately 3x speedup over the serial version as the size
            of the database increases, even with just 100 songs in the database.</p>
            
            <img class="responsive-img center-block" src="./images/svp-query2-2.png"/>
            
            <p>
              Similarly, we wanted to see how does the parallel implementation scale
              with a variable number of threads.  The following graph shows the performance 
              of the OpenMP implementation as we vary the number of threads used by the 
              program.  The database size for this benchmarking test was 100 songs, and
              the number of songs we queried for was 50 songs.
            </p>
            
            <img class="responsive-img center-block" src="./images/svp-query-3.png"/>
            
            <p>We see in the figure above that generally, as we increase the number of 
            threads spawned by OpenMP, the execution time of the parallel implementation 
            decreases.  We note two points of interest in the graph.  First, the single-threaded 
            OpenMP program has a higher execution time than just the single-threaded 
            implementation.  This result is not surprising, given that the overhead of 
            spawning a single pthread outweighs the benefit of just sequentially searching 
            through the database.  Second, the parallel version doesn't see any performance 
            gains beyond six threads.  The GHC 45 machine has six hyper-threaded cores, so 
            it makes sense that the optimal thread count is six (one thread per core), whereas
            trying to utilize both hyper-threaded execution contexts on each of the cores 
            results in some scheduling and context-switching overhead that limits performance.</p>
            
            <p>As a side note, we would have loved to have tested our implementation on 
            a database with thousands of songs; however, our implementation is only 
            capable of reading .WAV files, so we manually built our database of files 
            by converting MP3 files to WAV's and running them through our fingerprinting 
            algorithm.  We didn't come up with an efficient way to generate our database,
            otherwise we would have scaled to a larger number of songs.  </p>
            
            <p>We also think it would be interesting to see how the choice of 
            several key parameters affects the runtime and accuracy of our implementations.  
            For example, we chose a block size of 2048 samples to divide the input WAV file 
            into on which to perform each FFT.  Due to the nature of Fourier transforms, 
            decreasing the block size gives us higher-resolution time blocks, but also 
            gives us lower-resolution estimations of the frequencies that occur.  Conversely, 
            increasing the block size decreases the time fidelity, but provides greater 
            accuracy of the frequencies for that chunk of time.  Decreasing or increasing 
            the block size would affect the memory footprint of our audio fingerprints or 
            impact the accuracy of the similarity of each query and database comparison.  
            Another parameter that would be interesting to consider as well is the dimensions 
            of our histogram.  We determined the bin sizes by setting each bin to a range of 
            100 Hz, up to 1000 Hz.  Adding more bins would increase the accuracy of similarity
            matches, but also increase the memory footprint.  Alternatively, changing the 
            ranges to a logarithmic scale (30 - 40 Hz, 40 - 60 Hz, 60 - 100 Hz, 100 - 180 Hz,
            180 - 300 Hz, etc) might yield better results, due to the nature of the frequencies.  
            If we double the frequency f, we obtain the same music note one octave above the 
            original, so a logarithmic range might better distinguish between 
            individual frequencies.</p>
            
            <h5>Extending Our Implementation</h5>
            <p>We realized late that a GPU would have been an excellent candidate to perform
            our database lookups and fingerprint constructions.  But, we didn’t have the time
            to give it proper consideration because we were initially very focused on 
            implementing the algorithm on our Raspberry Pi, and then on the Gates 
            cluster machines.  The algorithm is amenable to both SIMD execution and GPU 
            execution.  We can divide the database of songs among gangs or thread blocks, 
            and each worker in the gang or thread in the thread block and be responsible 
            for computing the similarity of a given database song to the query song.  
            We could have furthered optimized this approach by storing the query song in 
            shared thread block memory for the GPU, for example, to take advantage of the 
            low latency access time of shared memory and further reduce the query match 
            execution time.  However, with our current database size, we would be no where close to 
            utilizing the GPU at its peak utilization.  This extension of the algorithm is 
            best suited for extending to larger databases for lookups.</p>
            
            <p>Secondly, we think our implemention could be extended to real time analysis,
            like we had originally planned, since the amount of time to generate a fingerprint 
            for an entire song is already so low.  Computing the fingerprint for a short clip of 
            a song would presumably have low latency, as we see in one of the earlier graphs
            showing the performance of the fingerprint generation algorithm.  However, 
            trying to match clips of songs to our database songs would add an additional 
            challenge of locating where in the database song the query song clip might occur. 
            This involves scanning over the song to find where the max similarity occurs, 
            which adds some additional work to our general search algorithm.  One consideration
            for this would be to parallelize the inter-song search with threads starting at 
            different points in the audio fingerprint, and searching for a max similarity.</p>
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="references" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <h4>References</h4>
            <p>We did a lot of reading online about digital signal processing, but the 
            following resources were the most helpful in developing and implementing our algorithm:</p>
            <ol>
              <li>Our course staff advisor <a class="tooltipped" data-position="top" data-delay="50" data-tooltip="Thank you!">Karima Ma</a> and <a class="tooltipped" data-position="top" data-delay="50" data-tooltip="And thank you!">Professor Kayvon</a></li>
              <li><a href="http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf" target=blank>"An Industrial-Strength Audio Search Algorithm"</a> by Avery Wang (2003) </li>
              <li><a href="https://www.toptal.com/algorithms/shazam-it-music-processing-fingerprinting-and-recognition" target=blank>"Shazam It! Music Recognition Algorithms, Fingerprinting, and Processing"</a> by Jovan Jovanovic</li>
              <li>This <a href="https://youtu.be/WhXgpkQ8E-Q" target="blank">tech talk</a> given by Peter Sobot, on An Industrial-Strength Audio Search Algorithm</li>
              <li><a href="http://coding-geek.com/how-shazam-works/" target="blank">"How does Shazam work"</a> on Coding-Geek (2015)</li>
            </ol>
            
          </div>
        </div>
      </div>
    </div>
    
    <div class="container">
      <div id="work" class="section scrollspy">
        <div class="row">
          <div class="col s12 m12">
            <p>Equal work was performed by both group members.</p>
          </div>
        </div>
      </div>
    </div>
  </main>

  <footer class="page-footer blue darken-2">
    <div class="container">
      <div class="row">
        <div class="col l6 s12">
          <h5 class="white-text">15-418 Final Project</h5>
          <h6 class="white-text">Kazam: Parallel Audio Recognition</h6>
          <ul>
            <li class="white-text">Casey Fischer (cjfische)</li>
            <li class="white-text">Udaya Malik (umalik)</li>
            <li><a class="orange-text text-lighten-3" href="https://github.com/caseyfisch/15418-final" target=blank>View Source on Github</a></li>
          </ul>
        </div>
      </div>
    </div>
    <div class="footer-copyright">
      <div class="container">
      Made by <a class="orange-text text-lighten-3" href="http://materializecss.com" target=blank>Materialize</a>
      </div>
    </div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>
